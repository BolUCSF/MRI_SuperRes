{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing zero checkpoint './deepspeed_checkpoint_superformer/SuperFormer_latest/latest_step'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 1078.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing checkpoint created by deepspeed==0.16.5\n",
      "Reconstructed fp32 state dict with 262 params 18232753 elements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving checkpoint shards: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from deepspeed.utils.zero_to_fp32 import convert_zero_checkpoint_to_fp32_state_dict\n",
    "checkpoint_path = './checkpoint_superformer'\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "model_path = './deepspeed_checkpoint_superformer/SuperFormer_latest'\n",
    "convert_zero_checkpoint_to_fp32_state_dict(model_path, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, torch, random\n",
    "import numpy as np\n",
    "from monai import transforms\n",
    "from monai.data import DataLoader, DistributedSampler\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from monai.networks.nets.swin_unetr import SwinUNETR\n",
    "from monai.inferers import sliding_window_inference\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "from models.SuperFormer import SuperFormer\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    def downsample_mri_kspace(self, mri_image, downsampling_factor):\n",
    "        \"\"\"\n",
    "        Downsamples an MRI image using k-space zero-filling.\n",
    "        \"\"\"\n",
    "        # Get the image data and affine transformation\n",
    "        data = mri_image[0]\n",
    "\n",
    "        # Get the spatial dimensions\n",
    "        spatial_dims = data.shape[:len(downsampling_factor)]\n",
    "        num_spatial_dims = len(spatial_dims)\n",
    "\n",
    "        # Check if the downsampling factor is valid\n",
    "        if len(downsampling_factor) != num_spatial_dims:\n",
    "            raise ValueError(f\"Downsampling factor length ({len(downsampling_factor)}) must match the number of spatial dimensions ({num_spatial_dims}).\")\n",
    "        for factor in downsampling_factor:\n",
    "            if not isinstance(factor, int) or factor < 1:\n",
    "                raise ValueError(\"Downsampling factors must be positive integers.\")\n",
    "\n",
    "        # Perform k-space transform\n",
    "        k_space = np.fft.fftn(data, axes=range(num_spatial_dims))\n",
    "        k_space_shifted = np.fft.fftshift(k_space, axes=range(num_spatial_dims))\n",
    "\n",
    "        # Create a new k-space array with zero-filling\n",
    "        new_k_space_shape = list(k_space_shifted.shape)\n",
    "\n",
    "        # Determine the central portion to keep in k-space\n",
    "        start_indices = []\n",
    "        end_indices = []\n",
    "        for i in range(num_spatial_dims):\n",
    "            center = spatial_dims[i] // 2\n",
    "            half_kept = (spatial_dims[i] // downsampling_factor[i]) // 2\n",
    "            start_indices.append(center - half_kept)\n",
    "            end_indices.append(center + (spatial_dims[i] // downsampling_factor[i]) - half_kept)\n",
    "            new_k_space_shape[i] = spatial_dims[i] // downsampling_factor[i]\n",
    "\n",
    "        # Place the central portion of the original k-space into the new (larger) array\n",
    "        slices = tuple(slice(start, end) for start, end in zip(start_indices, end_indices))\n",
    "        # downsampled_k_space_shifted[slices] = k_space_shifted[slices]\n",
    "        downsampled_k_space_shifted = k_space_shifted[slices]\n",
    "\n",
    "        # Inverse k-space transform to get the downsampled image\n",
    "        downsampled_k_space = np.fft.ifftshift(downsampled_k_space_shifted, axes=range(num_spatial_dims))\n",
    "        downsampled_data = np.fft.ifftn(downsampled_k_space, axes=range(num_spatial_dims)).real\n",
    "        downsampled_data = downsampled_data[np.newaxis, ...]  # Add a new axis to match the original shape\n",
    "\n",
    "        return downsampled_data\n",
    "\n",
    "    def __init__(self, image_paths, type, resample_factors):\n",
    "        self.image_paths = image_paths\n",
    "        self.resample_factors = resample_factors\n",
    "        self.train_transform_1 = transforms.Compose([\n",
    "            transforms.CopyItemsd(keys=[type], names=['path']),\n",
    "            transforms.LoadImaged(keys=[type]),\n",
    "            transforms.CopyItemsd(keys=[type], names=['image']),\n",
    "            transforms.DeleteItemsd(keys=[type]),\n",
    "            transforms.EnsureChannelFirstd(keys=[\"image\"]),\n",
    "            transforms.EnsureTyped(keys=[\"image\"]),\n",
    "            transforms.Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "            transforms.ScaleIntensityRangePercentilesd(keys=[\"image\"], lower=0.5, upper=99.5, b_min=0, b_max=1,clip=True ),\n",
    "            transforms.Spacingd(keys=['image'],pixdim=(1,1,1),mode=3),\n",
    "            transforms.ResizeWithPadOrCropd(keys=['image'],spatial_size=(240,240,160)),\n",
    "            transforms.CopyItemsd(keys=['image'], names=['hi_res','low_res'], times=2),\n",
    "            transforms.DeleteItemsd(keys=['image']),])\n",
    "        self.train_transform_2 = transforms.Compose([\n",
    "            transforms.Resized(keys=[\"low_res\"], spatial_size=(240,240,160), mode=\"nearest\"),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.image_paths[idx]\n",
    "        transformed_data = self.train_transform_1(data)\n",
    "        transformed_data['low_res'] = self.downsample_mri_kspace(transformed_data['low_res'],self.resample_factors)\n",
    "        transformed_data = self.train_transform_2(transformed_data)\n",
    "        return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('json/test_pair.json') as f:\n",
    "    test_pair = json.load(f)\n",
    "    print(len(test_pair))\n",
    "random.seed(0)\n",
    "random.shuffle(test_pair)\n",
    "test_pair = test_pair[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "down_sample = 4\n",
    "test_ds = MRIDataset(test_pair,type='fl', resample_factors = (1, 1, 4))\n",
    "test_loader = DataLoader(test_ds, batch_size=2, shuffle=False, num_workers=2, persistent_workers=True)#'path', 'hi_res', 'low_res'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SuperFormer(upscale=1,\n",
    "                   patch_size = 2,\n",
    "                   in_chans=1,\n",
    "                   img_size=64,\n",
    "                   window_size=8,\n",
    "                   img_range=1.0,\n",
    "                   depths=[6, 6, 6],\n",
    "                   embed_dim=240,\n",
    "                   num_heads=[6, 6, 6],\n",
    "                   mlp_ratio=2,\n",
    "                   upsampler=None,\n",
    "                   resi_connection=\"1conv\",\n",
    "                   ape=False, \n",
    "                   rpb=True,\n",
    "                   output_type = \"direct\",\n",
    "                   num_feat = 126)\n",
    "model.load_state_dict(torch.load('./checkpoint_superformer/pytorch_model.bin',weights_only=True))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model_inferer = partial(\n",
    "    sliding_window_inference,\n",
    "    roi_size=[64, 64, 64],\n",
    "    sw_batch_size=32,\n",
    "    predictor=model,\n",
    "    overlap=0.6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_res_folder = './Infer/high_res'\n",
    "low_res_folder = './Infer/low_res'\n",
    "recon_folder = './Infer/recon'\n",
    "if not os.path.exists(high_res_folder):\n",
    "    os.makedirs(high_res_folder)\n",
    "if not os.path.exists(low_res_folder):\n",
    "    os.makedirs(low_res_folder)\n",
    "if not os.path.exists(recon_folder):\n",
    "    os.makedirs(recon_folder)\n",
    "output_list = []\n",
    "with torch.no_grad():\n",
    "    with torch.autocast(device_type=\"cuda\" , dtype=torch.bfloat16):\n",
    "        for batch_data in tqdm(test_loader):\n",
    "            paths = batch_data[\"path\"]\n",
    "            image = batch_data[\"hi_res\"].to(device)\n",
    "            low_res = batch_data[\"low_res\"].to(device)\n",
    "            output = model_inferer(low_res)\n",
    "            high_res = image.cpu().float().numpy()\n",
    "            low_res = low_res.cpu().float().numpy()\n",
    "            recon = output.cpu().float().numpy()\n",
    "            B,C,H,W,D = high_res.shape\n",
    "            for i in range(B):\n",
    "                path = paths[i]\n",
    "                basename = os.path.basename(path)\n",
    "                tnum = path.split('/')[-1].split('_')[0]\n",
    "                low_res_image = low_res[i,0,:,:,:]\n",
    "                recon_image = recon[i,0,:,:,:]\n",
    "                high_res_image = high_res[i,0,:,:,:]\n",
    "                \n",
    "                raw_nib = nib.load(path)\n",
    "                spacing = raw_nib.header['pixdim'][1:4]\n",
    "                nib_size = raw_nib.shape\n",
    "                affine = np.eye(4)\n",
    "                affine[:3,:3] = np.diag(spacing)\n",
    "                transfrom_infer = transforms.Compose([\n",
    "                    transforms.EnsureChannelFirst(channel_dim=0),\n",
    "                    transforms.Spacing(pixdim=spacing,mode='bilinear'),\n",
    "                    transforms.ResizeWithPadOrCrop(spatial_size=nib_size),\n",
    "                    ])\n",
    "                low_res_image = transfrom_infer(low_res_image[np.newaxis,:,:,:].copy())[0].numpy()\n",
    "                recon_image = transfrom_infer(recon_image[np.newaxis,:,:,:].copy())[0].numpy()\n",
    "                high_res_image = transfrom_infer(high_res_image[np.newaxis,:,:,:].copy())[0].numpy()\n",
    "\n",
    "                low_res_nib = nib.Nifti1Image(low_res_image.astype(np.float32),affine)\n",
    "                recon_nib = nib.Nifti1Image(recon_image.astype(np.float32),affine)\n",
    "                high_res_nib = nib.Nifti1Image(high_res_image.astype(np.float32),affine)\n",
    "                hig_path = f'{high_res_folder}/{tnum}'\n",
    "                os.makedirs(hig_path,exist_ok=True)\n",
    "                low_path = f'{low_res_folder}_{down_sample}/{tnum}'\n",
    "                os.makedirs(low_path,exist_ok=True)\n",
    "                recon_path = f'{recon_folder}_{down_sample}/{tnum}'\n",
    "                os.makedirs(recon_path,exist_ok=True)\n",
    "\n",
    "                nib.save(high_res_nib,os.path.join(hig_path,basename))\n",
    "                nib.save(low_res_nib,os.path.join(low_path,basename))\n",
    "                nib.save(recon_nib,os.path.join(recon_path,basename))\n",
    "                output_list.append({'high_res':os.path.join(hig_path,basename),'low_res':os.path.join(low_path,basename),'recon':os.path.join(recon_path,basename)})\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "slice = 100\n",
    "image_ = high_res[idx][0]\n",
    "low_res_ = low_res[idx][0]\n",
    "recon_ = recon[idx][0]\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "plt.subplot(131)\n",
    "image_ = np.rot90(image_[:, slice, :])\n",
    "plt.imshow(image_, cmap='gray')\n",
    "plt.title('Original High Resolution', fontsize=16)\n",
    "\n",
    "plt.subplot(132)\n",
    "low_res_ = np.rot90(low_res_[:, slice, :])\n",
    "plt.imshow(low_res_, cmap='gray')\n",
    "plt.title('Low Resolution', fontsize=16)\n",
    "\n",
    "plt.subplot(133)\n",
    "recon_ = np.rot90(recon_[:, slice, :])\n",
    "plt.imshow(recon_, cmap='gray')\n",
    "plt.title('Super-resolution Image', fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from glob import glob\n",
    "\n",
    "def compute_psnr_ssim(lr_image: np.ndarray, hr_image: np.ndarray):\n",
    "    psnr_value = psnr(hr_image, lr_image, data_range=hr_image.max() - hr_image.min())\n",
    "    ssim_value = ssim(hr_image, lr_image, data_range=hr_image.max() - hr_image.min(), multichannel=True)\n",
    "    return psnr_value, ssim_value\n",
    "\n",
    "def compute_score_tnum_(tnum, down_sample=5, type='t1c'):\n",
    "    high_res_path = f'/data/Sup_Res/high_res/{tnum}'\n",
    "    low_res_path = f'/data/Sup_Res/low_res_{down_sample}/{tnum}'\n",
    "    recon_path = f'/data/Sup_Res/recon_{down_sample}/{tnum}'\n",
    "    print(f'{high_res_path}/*{type}_bet.nii.gz')\n",
    "    high_res = glob(f'{high_res_path}/*{type}_bet.nii.gz')[0]\n",
    "    low_res = glob(f'{low_res_path}/*{type}_bet.nii.gz')[0]\n",
    "    recon = glob(f'{recon_path}/*{type}_bet.nii.gz')[0]\n",
    "    high_res_nib = nib.load(high_res)\n",
    "    high_res_image = high_res_nib.get_fdata()\n",
    "    low_res_nib = nib.load(low_res)\n",
    "    low_res_image = low_res_nib.get_fdata()\n",
    "    recon_nib = nib.load(recon)\n",
    "    recon_image = recon_nib.get_fdata()\n",
    "    psnr_value, ssim_value = compute_psnr_ssim(low_res_image, high_res_image)\n",
    "    low_score = {'psnr':psnr_value, 'ssim':ssim_value}\n",
    "    psnr_value, ssim_value = compute_psnr_ssim(recon_image, high_res_image)\n",
    "    recon_score = {'psnr':psnr_value, 'ssim':ssim_value}\n",
    "    return low_score, recon_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score_tnum(high_path, low_path):\n",
    "    high_res_nib = nib.load(high_path)\n",
    "    high_res_image = high_res_nib.get_fdata()\n",
    "    low_res_nib = nib.load(low_path)\n",
    "    low_res_image = low_res_nib.get_fdata()\n",
    "\n",
    "    psnr_value, ssim_value = compute_psnr_ssim(low_res_image, high_res_image)\n",
    "    return psnr_value, ssim_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_downsample(input_path, output_path, down_sample):\n",
    "    nifti_img = nib.load(input_path)\n",
    "    data = nifti_img.get_fdata()\n",
    "    affine = nifti_img.affine.copy()\n",
    "\n",
    "    # 选择每 5 层中的 1 层\n",
    "    downsampled_data = data[:, :, ::down_sample]\n",
    "\n",
    "    # 调整 affine 矩阵，使 Z 轴间距变为原来的 5 倍\n",
    "    affine[2, :3] *= down_sample\n",
    "\n",
    "    # 创建新的 NIfTI 图像并保存\n",
    "    new_nifti = nib.Nifti1Image(downsampled_data, affine)\n",
    "    nib.save(new_nifti, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import zoom\n",
    "def process_nifti(input_path, output_path, downsample_factor=5):\n",
    "    # 加载NIfTI文件\n",
    "    img = nib.load(input_path)\n",
    "    data = img.get_fdata()\n",
    "    \n",
    "    # 获取原始维度\n",
    "    orig_shape = data.shape\n",
    "    \n",
    "    # 沿z轴每5层取1层\n",
    "    z_downsampled = data[:, :, ::downsample_factor]\n",
    "    \n",
    "    # 计算需要放大的倍数以恢复原始z轴大小\n",
    "    zoom_factor = orig_shape[2] / z_downsampled.shape[2]\n",
    "    \n",
    "    # 使用最近邻插值恢复到原始大小\n",
    "    # 对x,y轴保持不变(zoom_factor=1)，对z轴进行插值\n",
    "    resized_data = zoom(z_downsampled, \n",
    "                       (1, 1, zoom_factor), \n",
    "                       order=0)  # order=0表示最近邻插值\n",
    "    \n",
    "    # 确保输出维度与原始维度相同\n",
    "    if resized_data.shape != orig_shape:\n",
    "        # 如果有微小差异，进行裁剪或填充\n",
    "        resized_data = resized_data[:orig_shape[0], \n",
    "                                 :orig_shape[1], \n",
    "                                 :orig_shape[2]]\n",
    "    \n",
    "    # 创建新的NIfTI图像，保持原始的头信息\n",
    "    new_img = nib.Nifti1Image(resized_data, img.affine, img.header)\n",
    "    \n",
    "    # 保存到新的文件\n",
    "    nib.save(new_img, output_path)\n",
    "    print(f\"已保存处理后的文件到: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "down_sample = 3\n",
    "for input_path in glob('/data/Sup_Res/high_res/t*/*t1c_bet.nii.gz'):\n",
    "    output_path = input_path.replace('t1c_bet',f't1c_bet_{down_sample}')\n",
    "    process_nifti(input_path, output_path, down_sample)\n",
    "down_sample = 5\n",
    "for input_path in glob('/data/Sup_Res/high_res/t*/*fl_bet.nii.gz'):\n",
    "    output_path = input_path.replace('fl_bet',f'fl_bet_{down_sample}')\n",
    "    process_nifti(input_path, output_path, down_sample)\n",
    "down_sample = 5\n",
    "for input_path in glob('/data/Sup_Res/high_res/t*/*t1c_bet.nii.gz'):\n",
    "    output_path = input_path.replace('t1c_bet',f't1c_bet_{down_sample}')\n",
    "    process_nifti(input_path, output_path, down_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_value_list = []\n",
    "ssim_value_list = []\n",
    "for tnum in tqdm(os.listdir('/data/Sup_Res/high_res')):\n",
    "    # print(tnum)\n",
    "    high_res_path = glob(f'/data/Sup_Res/high_res/{tnum}/*fl_bet.nii.gz')[0]\n",
    "    low_res_path = glob(f'/data/Sup_Res/high_res/{tnum}/*fl_bet_3.nii.gz')[0]\n",
    "    # print(high_res_path, low_res_path)\n",
    "    psnr_value, ssim_value = compute_score_tnum(high_res_path, low_res_path)\n",
    "    # print(f'low_res: {psnr_value}, {ssim_value}')\n",
    "    psnr_value_list.append(psnr_value)\n",
    "    ssim_value_list.append(ssim_value)\n",
    "print('Flair downsample 3')\n",
    "print(np.mean(psnr_value_list), np.mean(ssim_value_list))\n",
    "print(np.std(psnr_value_list), np.std(ssim_value_list))\n",
    "score_data = {'psnr':psnr_value_list, 'ssim':ssim_value_list}\n",
    "pickle.dump(score_data,open('flair_downsample_3.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_value_list = []\n",
    "ssim_value_list = []\n",
    "for tnum in tqdm(os.listdir('/data/Sup_Res/high_res')):\n",
    "    # print(tnum)\n",
    "    high_res_path = glob(f'/data/Sup_Res/high_res/{tnum}/*fl_bet.nii.gz')[0]\n",
    "    low_res_path = glob(f'/data/Sup_Res/high_res/{tnum}/*fl_bet_5.nii.gz')[0]\n",
    "    # print(high_res_path, low_res_path)\n",
    "    psnr_value, ssim_value = compute_score_tnum(high_res_path, low_res_path)\n",
    "    # print(f'low_res: {psnr_value}, {ssim_value}')\n",
    "    psnr_value_list.append(psnr_value)\n",
    "    ssim_value_list.append(ssim_value)\n",
    "print('Flair downsample 5')\n",
    "print(np.mean(psnr_value_list), np.mean(ssim_value_list))\n",
    "print(np.std(psnr_value_list), np.std(ssim_value_list))\n",
    "score_data = {'psnr':psnr_value_list, 'ssim':ssim_value_list}\n",
    "pickle.dump(score_data,open('flair_downsample_5.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_value_list = []\n",
    "ssim_value_list = []\n",
    "for tnum in tqdm(os.listdir('/data/Sup_Res/high_res')):\n",
    "    # print(tnum)\n",
    "    high_res_path = glob(f'/data/Sup_Res/high_res/{tnum}/*t1c_bet.nii.gz')[0]\n",
    "    low_res_path = glob(f'/data/Sup_Res/high_res/{tnum}/*t1c_bet_3.nii.gz')[0]\n",
    "    # print(high_res_path, low_res_path)\n",
    "    psnr_value, ssim_value = compute_score_tnum(high_res_path, low_res_path)\n",
    "    # print(f'low_res: {psnr_value}, {ssim_value}')\n",
    "    psnr_value_list.append(psnr_value)\n",
    "    ssim_value_list.append(ssim_value)\n",
    "print('T1c downsample 3')\n",
    "print(np.mean(psnr_value_list), np.mean(ssim_value_list))\n",
    "print(np.std(psnr_value_list), np.std(ssim_value_list))\n",
    "score_data = {'psnr':psnr_value_list, 'ssim':ssim_value_list}\n",
    "pickle.dump(score_data,open('t1c_downsample_3.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_value_list = []\n",
    "ssim_value_list = []\n",
    "for tnum in tqdm(os.listdir('/data/Sup_Res/high_res')):\n",
    "    # print(tnum)\n",
    "    high_res_path = glob(f'/data/Sup_Res/high_res/{tnum}/*t1c_bet.nii.gz')[0]\n",
    "    low_res_path = glob(f'/data/Sup_Res/high_res/{tnum}/*t1c_bet_5.nii.gz')[0]\n",
    "    # print(high_res_path, low_res_path)\n",
    "    psnr_value, ssim_value = compute_score_tnum(high_res_path, low_res_path)\n",
    "    # print(f'low_res: {psnr_value}, {ssim_value}')\n",
    "    psnr_value_list.append(psnr_value)\n",
    "    ssim_value_list.append(ssim_value)\n",
    "print('T1c downsample 5')\n",
    "print(np.mean(psnr_value_list), np.mean(ssim_value_list))\n",
    "print(np.std(psnr_value_list), np.std(ssim_value_list))\n",
    "score_data = {'psnr':psnr_value_list, 'ssim':ssim_value_list}\n",
    "pickle.dump(score_data,open('t1c_downsample_5.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_value_list = []\n",
    "ssim_value_list = []\n",
    "for tnum in tqdm(os.listdir('/data/Sup_Res/high_res')):\n",
    "    # print(tnum)\n",
    "    high_res_path = glob(f'/data/Sup_Res/high_res/{tnum}/*t1c_bet.nii.gz')[0]\n",
    "    low_res_path = glob(f'/data/Sup_Res/low_res_5/{tnum}/*t1c_bet.nii.gz')[0]\n",
    "    # print(high_res_path, low_res_path)\n",
    "    psnr_value, ssim_value = compute_score_tnum(high_res_path, low_res_path)\n",
    "    # print(f'low_res: {psnr_value}, {ssim_value}')\n",
    "    psnr_value_list.append(psnr_value)\n",
    "    ssim_value_list.append(ssim_value)\n",
    "print('T1c bicubic 5')\n",
    "print(np.mean(psnr_value_list), np.mean(ssim_value_list))\n",
    "print(np.std(psnr_value_list), np.std(ssim_value_list))\n",
    "score_data = {'psnr':psnr_value_list, 'ssim':ssim_value_list}\n",
    "pickle.dump(score_data,open('t1c_bicubic_5.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_value_list = []\n",
    "ssim_value_list = []\n",
    "for tnum in tqdm(os.listdir('/data/Sup_Res/high_res')):\n",
    "    # print(tnum)\n",
    "    high_res_path = glob(f'/data/Sup_Res/high_res/{tnum}/*t1c_bet.nii.gz')[0]\n",
    "    low_res_path = glob(f'/data/Sup_Res/low_res_3/{tnum}/*t1c_bet.nii.gz')[0]\n",
    "    # print(high_res_path, low_res_path)\n",
    "    psnr_value, ssim_value = compute_score_tnum(high_res_path, low_res_path)\n",
    "    # print(f'low_res: {psnr_value}, {ssim_value}')\n",
    "    psnr_value_list.append(psnr_value)\n",
    "    ssim_value_list.append(ssim_value)\n",
    "print('T1c bicubic 3')\n",
    "print(np.mean(psnr_value_list), np.mean(ssim_value_list))\n",
    "print(np.std(psnr_value_list), np.std(ssim_value_list))\n",
    "score_data = {'psnr':psnr_value_list, 'ssim':ssim_value_list}\n",
    "pickle.dump(score_data,open('t1c_bicubic_3.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_value_list = []\n",
    "ssim_value_list = []\n",
    "for tnum in tqdm(os.listdir('/data/Sup_Res/high_res')):\n",
    "    # print(tnum)\n",
    "    high_res_path = glob(f'/data/Sup_Res/high_res/{tnum}/*fl_bet.nii.gz')[0]\n",
    "    low_res_path = glob(f'/data/Sup_Res/low_res_5/{tnum}/*fl_bet.nii.gz')[0]\n",
    "    # print(high_res_path, low_res_path)\n",
    "    psnr_value, ssim_value = compute_score_tnum(high_res_path, low_res_path)\n",
    "    # print(f'low_res: {psnr_value}, {ssim_value}')\n",
    "    psnr_value_list.append(psnr_value)\n",
    "    ssim_value_list.append(ssim_value)\n",
    "print('Flair bicubic 5')\n",
    "print(np.mean(psnr_value_list), np.mean(ssim_value_list))\n",
    "print(np.std(psnr_value_list), np.std(ssim_value_list))\n",
    "score_data = {'psnr':psnr_value_list, 'ssim':ssim_value_list}\n",
    "pickle.dump(score_data,open('fl_bicubic_5.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_value_list = []\n",
    "ssim_value_list = []\n",
    "for tnum in tqdm(os.listdir('/data/Sup_Res/high_res')):\n",
    "    # print(tnum)\n",
    "    high_res_path = glob(f'/data/Sup_Res/high_res/{tnum}/*fl_bet.nii.gz')[0]\n",
    "    low_res_path = glob(f'/data/Sup_Res/low_res_3/{tnum}/*fl_bet.nii.gz')[0]\n",
    "    # print(high_res_path, low_res_path)\n",
    "    psnr_value, ssim_value = compute_score_tnum(high_res_path, low_res_path)\n",
    "    # print(f'low_res: {psnr_value}, {ssim_value}')\n",
    "    psnr_value_list.append(psnr_value)\n",
    "    ssim_value_list.append(ssim_value)\n",
    "print('Flair bicubic 3')\n",
    "print(np.mean(psnr_value_list), np.mean(ssim_value_list))\n",
    "print(np.std(psnr_value_list), np.std(ssim_value_list))\n",
    "score_data = {'psnr':psnr_value_list, 'ssim':ssim_value_list}\n",
    "pickle.dump(score_data,open('fl_bicubic_3.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_value_list = []\n",
    "ssim_value_list = []\n",
    "for tnum in tqdm(os.listdir('/data/Sup_Res/high_res')):\n",
    "    # print(tnum)\n",
    "    high_res_path = glob(f'/data/Sup_Res/high_res/{tnum}/*t1c_bet.nii.gz')[0]\n",
    "    low_res_path = glob(f'/data/Sup_Res/recon_5/{tnum}/*t1c_bet.nii.gz')[0]\n",
    "    # print(high_res_path, low_res_path)\n",
    "    psnr_value, ssim_value = compute_score_tnum(high_res_path, low_res_path)\n",
    "    # print(f'low_res: {psnr_value}, {ssim_value}')\n",
    "    psnr_value_list.append(psnr_value)\n",
    "    ssim_value_list.append(ssim_value)\n",
    "print('T1c recon 5')\n",
    "print(np.mean(psnr_value_list), np.mean(ssim_value_list))\n",
    "print(np.std(psnr_value_list), np.std(ssim_value_list))\n",
    "score_data = {'psnr':psnr_value_list, 'ssim':ssim_value_list}\n",
    "pickle.dump(score_data,open('t1c_recon_5.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_value_list = []\n",
    "ssim_value_list = []\n",
    "for tnum in tqdm(os.listdir('/data/Sup_Res/high_res')):\n",
    "    # print(tnum)\n",
    "    high_res_path = glob(f'/data/Sup_Res/high_res/{tnum}/*t1c_bet.nii.gz')[0]\n",
    "    low_res_path = glob(f'/data/Sup_Res/recon_3/{tnum}/*t1c_bet.nii.gz')[0]\n",
    "    # print(high_res_path, low_res_path)\n",
    "    psnr_value, ssim_value = compute_score_tnum(high_res_path, low_res_path)\n",
    "    # print(f'low_res: {psnr_value}, {ssim_value}')\n",
    "    psnr_value_list.append(psnr_value)\n",
    "    ssim_value_list.append(ssim_value)\n",
    "print('T1c recon 3')\n",
    "print(np.mean(psnr_value_list), np.mean(ssim_value_list))\n",
    "print(np.std(psnr_value_list), np.std(ssim_value_list))\n",
    "score_data = {'psnr':psnr_value_list, 'ssim':ssim_value_list}\n",
    "pickle.dump(score_data,open('t1c_recon_3.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_value_list = []\n",
    "ssim_value_list = []\n",
    "for tnum in tqdm(os.listdir('/data/Sup_Res/high_res')):\n",
    "    # print(tnum)\n",
    "    high_res_path = glob(f'/data/Sup_Res/high_res/{tnum}/*fl_bet.nii.gz')[0]\n",
    "    low_res_path = glob(f'/data/Sup_Res/recon_5/{tnum}/*fl_bet.nii.gz')[0]\n",
    "    # print(high_res_path, low_res_path)\n",
    "    psnr_value, ssim_value = compute_score_tnum(high_res_path, low_res_path)\n",
    "    # print(f'low_res: {psnr_value}, {ssim_value}')\n",
    "    psnr_value_list.append(psnr_value)\n",
    "    ssim_value_list.append(ssim_value)\n",
    "print('Flair recon 5')\n",
    "print(np.mean(psnr_value_list), np.mean(ssim_value_list))\n",
    "print(np.std(psnr_value_list), np.std(ssim_value_list))\n",
    "score_data = {'psnr':psnr_value_list, 'ssim':ssim_value_list}\n",
    "pickle.dump(score_data,open('fl_recon_5.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_value_list = []\n",
    "ssim_value_list = []\n",
    "for tnum in tqdm(os.listdir('/data/Sup_Res/high_res')):\n",
    "    # print(tnum)\n",
    "    high_res_path = glob(f'/data/Sup_Res/high_res/{tnum}/*fl_bet.nii.gz')[0]\n",
    "    low_res_path = glob(f'/data/Sup_Res/recon_3/{tnum}/*fl_bet.nii.gz')[0]\n",
    "    # print(high_res_path, low_res_path)\n",
    "    psnr_value, ssim_value = compute_score_tnum(high_res_path, low_res_path)\n",
    "    # print(f'low_res: {psnr_value}, {ssim_value}')\n",
    "    psnr_value_list.append(psnr_value)\n",
    "    ssim_value_list.append(ssim_value)\n",
    "print('Flair recon 3')\n",
    "print(np.mean(psnr_value_list), np.mean(ssim_value_list))\n",
    "print(np.std(psnr_value_list), np.std(ssim_value_list))\n",
    "score_data = {'psnr':psnr_value_list, 'ssim':ssim_value_list}\n",
    "pickle.dump(score_data,open('fl_recon_3.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
